---
id: 68420c7fb5b9e36eefb9e98f
title: ¿Qué es la programación dinámica y cuáles son algunos algoritmos comunes?
challengeType: 19
dashedName: what-is-dynamic-programming-and-what-are-some-common-algorithms
---

# --description--
  
La programación dinámica es una técnica algorítmica que resuelve problemas complejos dividiéndolos en subproblemas más simples y almacenando los resultados para evitar cálculos redundantes. Este enfoque transforma problemas que normalmente tomarían tiempo exponencial en otros que pueden resolverse en tiempo polinomial.

## Principios fundamentales de la programación dinámica

La programación dinámica funciona cuando dos condiciones clave están presentes en un problema.

- **Subproblemas Superpuestos**: Los mismos problemas más pequeños aparecen varias veces al resolver el problema más grande. En lugar de recalcular estos subproblemas repetidamente, almacenamos sus soluciones.
    
- **Estructura Óptima**: La solución óptima al problema contiene soluciones óptimas a sus subproblemas. Esto significa que podemos construir la mejor solución combinando las mejores soluciones a partes más pequeñas.
    

Examinemos estos conceptos usando el clásico problema de "subir escaleras".

## El problema con la recursión ingenua

Considera el problema de subir escaleras: estás subiendo una escalera con `n` escalones y puedes subir 1 o 2 escalones a la vez. ¿Cuántas formas distintas hay para llegar a la cima?

```python
def climb_stairs_recursive(n):
    """Recursive approach"""
    if n <= 2:
        return n  # Base cases: 1 way for 1 step, 2 ways for 2 steps
    # To reach step n, we can come from step (n-1) or step (n-2)
    return climb_stairs_recursive(n-1) + climb_stairs_recursive(n-2)
```

Esta implementación tiene complejidad temporal exponencial debido a cálculos redundantes masivos. Al calcular `climb_stairs(5)`, esto es lo que sucede:

- `climb_stairs(5)` llama a `climb_stairs(4)` y `climb_stairs(3)`
    
- `climb_stairs(4)` llama a `climb_stairs(3)` y `climb_stairs(2)`
    
- Ahora `climb_stairs(3)` se calcula **dos veces**
    
- `climb_stairs(3)` llama a `climb_stairs(2)` y `climb_stairs(1)`
    
- `climb_stairs(2)` se calcula **3 veces en total**
    

Para solo `n=5`, hacemos 9 llamadas a funciones cuando solo necesitamos 5 cálculos únicos. A medida que `n` crece, esta redundancia explota exponencialmente: ¡`climb_stairs(30)` requeriría más de 1 billón de llamadas a funciones! La complejidad temporal se vuelve `O(2^n)`, lo que lo hace ineficiente e impráctico para valores mayores de `n`.

## Soluciones de Programación Dinámica

La programación dinámica elimina este cálculo redundante mediante dos enfoques principales:

### Memorización (Enfoque de arriba hacia abajo)

La memoización almacena los resultados de llamadas a funciones costosas y devuelve el resultado en caché cuando ocurren las mismas entradas nuevamente:

```python
def climb_stairs_memo(n, memo={}):
    """Dynamic programming with memoization"""
    # Check if we've already calculated this value
    if n in memo:
        return memo[n]  # Return cached result - O(1) lookup!
    
    # Base cases
    if n <= 2:
        return n
    
    # Calculate once and store in memo for future use
    memo[n] = climb_stairs_memo(n-1, memo) + climb_stairs_memo(n-2, memo)
    return memo[n]
```

La memoización es mucho más eficiente porque cada valor único desde `1` hasta `n` se calcula exactamente una vez. Cuando necesitamos `climb_stairs(3)` de nuevo, en lugar de recalcularlo (lo que desencadenaría más llamadas recursivas), simplemente lo buscamos en nuestro diccionario memo en tiempo `O(1)`.

Vamos a seguir la ejecución de `climb_stairs(5)` con el enfoque de arriba hacia abajo para ver cómo la memoización elimina el trabajo redundante:

```md
Call: climb_stairs_memo(5)
  memo = {} (empty)
  
  Call: climb_stairs_memo(4) 
    memo = {} (empty)
    
    Call: climb_stairs_memo(3)
      memo = {} (empty)
      
      Call: climb_stairs_memo(2) → returns 2 (base case)
      Call: climb_stairs_memo(1) → returns 1 (base case)
      
      Result: 2 + 1 = 3
      memo = {3: 3} (stored!)
    
    Call: climb_stairs_memo(2) → returns 2 (base case)
    
    Result: 3 + 2 = 5
    memo = {3: 3, 4: 5} (stored!)
  
  Call: climb_stairs_memo(3) → returns 3 (FROM MEMO - no recursion!)
  
  Result: 5 + 3 = 8
  memo = {3: 3, 4: 5, 5: 8}
```

**Comparación de eficiencia**

- **Recursivo ingenuo**: Hace 9 llamadas a funciones con cálculos repetidos
    
- **Memorización**: Realiza solo 5 cálculos únicos y luego reutiliza los resultados almacenados
    
- **Complejidad temporal**: Reducida de `O(2^n)` a `O(n)` ya que hacemos solo `n` cálculos únicos
    
- **Complejidad espacial**: `O(n)` para el almacenamiento de memo y la pila de llamadas
    
- **Impacto real**: `climb_stairs(30)` pasa de más de 1 mil millones de llamadas a solo 30 llamadas.
    

### Tabulación (Enfoque de abajo hacia arriba)

La tabulación construye la solución desde cero, llenando una tabla con soluciones a subproblemas:

```python
def climb_stairs_tabulation(n):
    """Dynamic programming with tabulation"""
    if n <= 2:
        return n
    
    # Create array to store results for all steps from 0 to n
    dp = [0] * (n + 1)
    dp[1] = 1  # 1 way to reach step 1
    dp[2] = 2  # 2 ways to reach step 2
    
    # Build up the solution iteratively
    for i in range(3, n + 1):
        # Ways to reach step i = ways to reach (i-1) + ways to reach (i-2)
        dp[i] = dp[i-1] + dp[i-2]
    
    return dp[n]
```

La tabulación elimina la recursión por completo al construir la solución de manera iterativa desde los subproblemas más pequeños hasta el objetivo.

Veamos el enfoque de abajo hacia arriba en acción para ver cómo construimos la solución de manera sistemática. Aquí está la estructura iterativa para `climb_stairs(5)`:

```md
Initial state:
dp = [0, 1, 2, 0, 0, 0]
     [0, 1, 2, 3, 4, 5] ← indices (step numbers)

Step by step construction:

i = 3:
  dp[3] = dp[2] + dp[1] = 2 + 1 = 3
  dp = [0, 1, 2, 3, 0, 0]
  
i = 4:
  dp[4] = dp[3] + dp[2] = 3 + 2 = 5
  dp = [0, 1, 2, 3, 5, 0]
  
i = 5:
  dp[5] = dp[4] + dp[3] = 5 + 3 = 8
  dp = [0, 1, 2, 3, 5, 8]

Final result: dp[5] = 8
```

**Ventajas clave de la tabulación**

- **Sin sobrecarga de recursión**: A diferencia de la memorización, no hay pila de llamadas recursivas.
    
- **Ejecución predecible**: Calculamos valores en un orden predeterminado (1, 2, 3, 4, 5...).
    
- **Amigable con la caché**: El acceso secuencial a arrays optimiza el uso de memoria.
    
- **Fácil de optimizar**: Puede reducir la complejidad espacial a `O(1)` ya que solo necesitamos los dos últimos valores.
    

```python
def climb_stairs_optimized(n):
    if n <= 2:
        return n
    
    prev2, prev1 = 1, 2  # Only store last two values
    for i in range(3, n + 1):
        current = prev1 + prev2
        prev2, prev1 = prev1, current
    return prev1
```

**Comparación de eficiencia**

- **Recursivo ingenuo**: 9 llamadas a función para `n=5`, crecimiento exponencial.
    
- **Tabulación**: 3 sumas simples para `n=5`, crecimiento lineal.
    
- **Complejidad temporal**: `O(n)` en lugar de `O(2^n)`.
    
- **Complejidad espacial**: `O(n)` para el arreglo, o `O(1)` con optimización.
    
- **Rendimiento predecible**: Sin riesgo de desbordamiento de pila para entradas grandes.
    

Ambos enfoques reducen la complejidad temporal de exponencial `O(2^n)` a lineal `O(n)`, una mejora drástica que marca la diferencia entre resolver el problema en milisegundos o esperar años para entradas más grandes.

## Aplicaciones en el mundo real

La programación dinámica tiene aplicaciones generalizadas en la informática y más allá:

- **Optimización de Rutas**: Los sistemas GPS usan algoritmos de programación dinámica para encontrar los caminos más cortos entre ubicaciones.
    
- **Procesamiento de Texto**: Los correctores ortográficos y las funciones de autocompletado a menudo dependen de la programación dinámica para calcular las distancias de edición entre palabras.
    
- **Modelado Financiero**: Las estrategias de inversión y la optimización de portafolios frecuentemente emplean técnicas de programación dinámica.
    
- **Asignación de Recursos**: El problema de la mochila y sus variantes aparecen en la programación, presupuestación y gestión de recursos.
    

## Ejemplo Práctico: Problema del Cambio de Monedas

El problema del cambio de monedas es un desafío clásico de programación que, cuando se resuelve usando programación dinámica, demuestra ambos principios clave de DP: estructura óptima y subproblemas superpuestos.

El problema de cambio de monedas pregunta: "¿Cuál es el número mínimo de monedas necesarias para alcanzar una cantidad objetivo?"

Aquí tienes una solución usando programación dinámica:

```python
def min_coins(amount, coins):
    """Find minimum number of coins needed to make the given amount"""
    # Initialize dp array with "infinity" - represents impossible to make
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0  # Base case: 0 coins needed for amount 0
    
    # For each amount from 1 to target amount
    for i in range(1, amount + 1):
        # Try each coin denomination
        for coin in coins:
            if coin <= i:  # Can only use coin if it doesn't exceed current amount
                # Update minimum: current minimum vs (coins for remaining amount + 1)
                dp[i] = min(dp[i], dp[i - coin] + 1)
    
    # Return result if possible, -1 if impossible
    return dp[amount] if dp[amount] != float('inf') else -1

# Example usage:
# coins = [1, 3, 4], amount = 6
# dp[6] = min(dp[5]+1, dp[3]+1, dp[2]+1) = min(3+1, 1+1, 2+1) = 2
# Result: 2 coins (3 + 3)
```

Y aquí está cómo funciona el algoritmo de cambio de monedas con programación dinámica paso a paso para `coins = [1, 3, 4]`, `amount = 6`:

```md
Initial state:
dp = [0, ∞, ∞, ∞, ∞, ∞, ∞]
     [0, 1, 2, 3, 4, 5, 6] ← amounts

Building up the solution:

For amount = 1:
  Try coin 1: dp[1] = min(∞, dp[0] + 1) = min(∞, 0 + 1) = 1
  dp = [0, 1, ∞, ∞, ∞, ∞, ∞]

For amount = 2:
  Try coin 1: dp[2] = min(∞, dp[1] + 1) = min(∞, 1 + 1) = 2
  dp = [0, 1, 2, ∞, ∞, ∞, ∞]

For amount = 3:
  Try coin 1: dp[3] = min(∞, dp[2] + 1) = min(∞, 2 + 1) = 3
  Try coin 3: dp[3] = min(3, dp[0] + 1) = min(3, 0 + 1) = 1
  dp = [0, 1, 2, 1, ∞, ∞, ∞]

For amount = 4:
  Try coin 1: dp[4] = min(∞, dp[3] + 1) = min(∞, 1 + 1) = 2
  Try coin 3: dp[4] = min(2, dp[1] + 1) = min(2, 1 + 1) = 2
  Try coin 4: dp[4] = min(2, dp[0] + 1) = min(2, 0 + 1) = 1
  dp = [0, 1, 2, 1, 1, ∞, ∞]

For amount = 5:
  Try coin 1: dp[5] = min(∞, dp[4] + 1) = min(∞, 1 + 1) = 2
  Try coin 3: dp[5] = min(2, dp[2] + 1) = min(2, 2 + 1) = 2
  Try coin 4: dp[5] = min(2, dp[1] + 1) = min(2, 1 + 1) = 2
  dp = [0, 1, 2, 1, 1, 2, ∞]

For amount = 6:
  Try coin 1: dp[6] = min(∞, dp[5] + 1) = min(∞, 2 + 1) = 3
  Try coin 3: dp[6] = min(3, dp[3] + 1) = min(3, 1 + 1) = 2
  Try coin 4: dp[6] = min(2, dp[2] + 1) = min(2, 2 + 1) = 2
  dp = [0, 1, 2, 1, 1, 2, 2]

Final result: dp[6] = 2 (achieved with coins 3 + 3)
```

Esta solución demuestra ambos principios clave de la programación dinámica. Tiene subproblemas superpuestos porque encontrar las monedas mínimas para la cantidad 6 requiere conocer las soluciones para las cantidades 5, 3 y 2. Estos mismos subproblemas aparecen al calcular otras cantidades. Tiene estructura óptima porque la solución óptima para cualquier cantidad incorpora soluciones óptimas para cantidades menores. Si sabemos que las monedas mínimas para la cantidad 3 es 1, entonces una forma de hacer la cantidad 6 es usar esa solución más una moneda adicional de valor 3.

Sin DP, tendríamos que probar cada combinación posible de monedas, un número exponencial de posibilidades. Con DP, construimos la solución de manera sistemática:

- **Complejidad temporal**: `O(amount × number of coins)` en lugar de exponencial.
    
- **Complejidad espacial**: `O(amount)` para el arreglo `dp`.
    
- **No trabajo redundante**: Cada subproblema (encontrar la cantidad mínima de monedas para cada monto) se resuelve exactamente una vez.
    
- **Resultados reutilizables**: Una vez que sabemos las monedas mínimas para la cantidad 3, usamos este conocimiento para todas las cantidades mayores que puedan beneficiarse de ello.
    

## Cuándo usar programación dinámica

La programación dinámica es efectiva cuando:

- El problema puede dividirse en subproblemas superpuestos.
    
- El problema exhibe estructura óptima.
    
- Una solución recursiva ingenua implicaría cálculos repetidos.
    
- Necesitas optimizar para la complejidad temporal a costa de la complejidad espacial.
    

Los patrones comunes de programación dinámica incluyen problemas de optimización (encontrar valores mínimos/máximos), problemas de conteo (número de formas de lograr algo) y problemas de decisión que pueden descomponerse en decisiones más pequeñas.

La programación dinámica transforma problemas complejos en problemas manejables al almacenar y reutilizar sistemáticamente soluciones a subproblemas. Entender esta técnica abre la puerta para resolver una amplia gama de desafíos computacionales de manera eficiente.

# --questions--

## --text--

¿Cuáles son las dos condiciones clave que deben estar presentes para que la programación dinámica sea aplicable?

## --answers--

Ejecución rápida y bajo uso de memoria.

### --feedback--

Piensa en qué hace que un problema sea adecuado para la optimización mediante programación dinámica.

---

Subproblemas superpuestos y estructura óptima.

---

Llamadas recursivas y bucles iterativos.

### --feedback--

Piensa en qué hace que un problema sea adecuado para la optimización mediante programación dinámica.

---

Memorización y tabulación.

### --feedback--

Piensa en qué hace que un problema sea adecuado para la optimización mediante programación dinámica.

## --video-solution--

2

## --text--

¿Cuál es la diferencia principal entre los enfoques de memoización y tabulación en programación dinámica?

## --answers--

La memoización usa más memoria que la tabulación.

### --feedback--

Considera la dirección en la que cada estructura construye la solución.

---

La tabulación siempre es más rápida que la memorización.

### --feedback--

Considera la dirección en la que cada estructura construye la solución.

---

La memoización es de arriba hacia abajo mientras que la tabulación es de abajo hacia arriba.

---

La memoización solo funciona con funciones recursivas.

### --feedback--

Considera la dirección en la que cada estructura construye la solución.

## --video-solution--

3

## --text--

En la implementación ingenua recursiva para subir escaleras, ¿cuál es la complejidad temporal y por qué es ineficiente?

## --answers--

`O(n)` porque calcula cada paso una sola vez.

### --feedback--

Piensa en cuántas veces se calculan las mismas cuentas de escalones en el árbol recursivo.

---

`O(n²)` porque usa bucles anidados.

### --feedback--

Piensa en cuántas veces se calculan las mismas cuentas de escalones en el árbol recursivo.

---

`O(2^n)` porque recalcula los mismos subproblemas múltiples veces.

---

`O(log n)` porque divide el problema a la mitad.

### --feedback--

Piensa en cuántas veces se calculan las mismas cuentas de escalones en el árbol recursivo.

## --video-solution--

3
