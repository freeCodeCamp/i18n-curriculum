---
id: 68420c7fb5b9e36eefb9e98f
title: O que é Programação Dinâmica e Quais São Alguns Algoritmos Comuns?
challengeType: 19
dashedName: what-is-dynamic-programming-and-what-are-some-common-algorithms
---

# --description--
  
Programação dinâmica é uma técnica algorítmica que resolve problemas complexos dividindo-os em subproblemas mais simples e armazenando os resultados para evitar cálculos redundantes. Essa abordagem transforma problemas que normalmente levariam tempo exponencial em problemas que podem ser resolvidos em tempo polinomial.

## Princípios Fundamentais da Programação Dinâmica

Programação dinâmica funciona quando duas condições-chave estão presentes em um problema.

- **Subproblemas Sobrepostos**: Os mesmos problemas menores aparecem várias vezes ao resolver o problema maior. Em vez de recalcular esses subproblemas repetidamente, armazenamos suas soluções.
    
- **Subestrutura Ótima**: A solução ótima para o problema contém soluções ótimas para seus subproblemas. Isso significa que podemos construir a melhor solução combinando as melhores soluções para partes menores.
    

Vamos examinar esses conceitos usando o clássico problema de "subir escadas".

## O Problema com Recursão Ingênua

Considere o problema de subir escadas: você está subindo uma escada com `n` degraus e pode subir 1 ou 2 degraus por vez. De quantas maneiras distintas você pode chegar ao topo?

```python
def climb_stairs_recursive(n):
    """Recursive approach"""
    if n <= 2:
        return n  # Base cases: 1 way for 1 step, 2 ways for 2 steps
    # To reach step n, we can come from step (n-1) or step (n-2)
    return climb_stairs_recursive(n-1) + climb_stairs_recursive(n-2)
```

Esta implementação tem complexidade de tempo exponencial por causa de cálculos redundantes massivos. Ao calcular `climb_stairs(5)`, veja o que acontece:

- `climb_stairs(5)` chama `climb_stairs(4)` e `climb_stairs(3)`
    
- `climb_stairs(4)` chama `climb_stairs(3)` e `climb_stairs(2)`
    
- Agora `climb_stairs(3)` é calculado **duas vezes**
    
- `climb_stairs(3)` chama `climb_stairs(2)` e `climb_stairs(1)`
    
- `climb_stairs(2)` é calculado **3 vezes no total**
    

Para apenas `n=5`, fazemos 9 chamadas de função quando precisamos de apenas 5 cálculos únicos. À medida que `n` cresce, essa redundância explode exponencialmente - `climb_stairs(30)` exigiria mais de 1 bilhão de chamadas de função! A complexidade de tempo se torna `O(2^n)`, tornando-o ineficiente e impraticável para valores maiores de `n`.

## Soluções de Programação Dinâmica

A programação dinâmica elimina esse cálculo redundante por meio de duas abordagens principais:

### Memoização (Abordagem de Cima para Baixo)

A memoização armazena os resultados de chamadas de função caras e retorna o resultado em cache quando as mesmas entradas ocorrem novamente:

```python
def climb_stairs_memo(n, memo={}):
    """Dynamic programming with memoization"""
    # Check if we've already calculated this value
    if n in memo:
        return memo[n]  # Return cached result - O(1) lookup!
    
    # Base cases
    if n <= 2:
        return n
    
    # Calculate once and store in memo for future use
    memo[n] = climb_stairs_memo(n-1, memo) + climb_stairs_memo(n-2, memo)
    return memo[n]
```

A memoização é muito mais eficiente porque cada valor único de `1` a `n` é calculado exatamente uma vez. Quando precisamos de `climb_stairs(3)` novamente, em vez de recalculá-lo (o que acionaria mais chamadas recursivas), simplesmente o consultamos em nosso dicionário memo em tempo `O(1)`.

Vamos acompanhar a execução de `climb_stairs(5)` com a abordagem top-down para ver como a memoização elimina trabalhos redundantes:

```md
Call: climb_stairs_memo(5)
  memo = {} (empty)
  
  Call: climb_stairs_memo(4) 
    memo = {} (empty)
    
    Call: climb_stairs_memo(3)
      memo = {} (empty)
      
      Call: climb_stairs_memo(2) → returns 2 (base case)
      Call: climb_stairs_memo(1) → returns 1 (base case)
      
      Result: 2 + 1 = 3
      memo = {3: 3} (stored!)
    
    Call: climb_stairs_memo(2) → returns 2 (base case)
    
    Result: 3 + 2 = 5
    memo = {3: 3, 4: 5} (stored!)
  
  Call: climb_stairs_memo(3) → returns 3 (FROM MEMO - no recursion!)
  
  Result: 5 + 3 = 8
  memo = {3: 3, 4: 5, 5: 8}
```

**Comparação de eficiência**

- **Recursivo ingênuo**: Faz 9 chamadas de função com cálculos repetidos
    
- **Memoização**: Realiza apenas 5 cálculos únicos, depois reutiliza os resultados armazenados
    
- **Complexidade de tempo**: Reduzida de `O(2^n)` para `O(n)` já que fazemos apenas `n` cálculos únicos
    
- **Complexidade de espaço**: `O(n)` para o armazenamento do memo e a pilha de chamadas
    
- **Impacto real**: `climb_stairs(30)` cai de mais de 1 bilhão de chamadas para apenas 30 chamadas!
    

### Tabulação (Abordagem Bottom-Up)

A tabulação constrói a solução do zero, preenchendo uma tabela com soluções para subproblemas:

```python
def climb_stairs_tabulation(n):
    """Dynamic programming with tabulation"""
    if n <= 2:
        return n
    
    # Create array to store results for all steps from 0 to n
    dp = [0] * (n + 1)
    dp[1] = 1  # 1 way to reach step 1
    dp[2] = 2  # 2 ways to reach step 2
    
    # Build up the solution iteratively
    for i in range(3, n + 1):
        # Ways to reach step i = ways to reach (i-1) + ways to reach (i-2)
        dp[i] = dp[i-1] + dp[i-2]
    
    return dp[n]
```

A tabulação elimina a recursão completamente ao construir a solução iterativamente desde os menores subproblemas até o alvo.

Vamos ver a abordagem bottom-up em ação para entender como construímos a solução de forma sistemática. Aqui está a construção iterativa para `climb_stairs(5)`:

```md
Initial state:
dp = [0, 1, 2, 0, 0, 0]
     [0, 1, 2, 3, 4, 5] ← indices (step numbers)

Step by step construction:

i = 3:
  dp[3] = dp[2] + dp[1] = 2 + 1 = 3
  dp = [0, 1, 2, 3, 0, 0]
  
i = 4:
  dp[4] = dp[3] + dp[2] = 3 + 2 = 5
  dp = [0, 1, 2, 3, 5, 0]
  
i = 5:
  dp[5] = dp[4] + dp[3] = 5 + 3 = 8
  dp = [0, 1, 2, 3, 5, 8]

Final result: dp[5] = 8
```

**Principais vantagens da tabulação**

- **Sem sobrecarga de recursão**: Ao contrário da memoização, não há pilha de chamadas recursivas.
    
- **Execução previsível**: Calculamos valores em uma ordem predeterminada (1, 2, 3, 4, 5...).
    
- **Amigável ao cache**: O acesso sequencial ao array otimiza o uso da memória.
    
- **Fácil de otimizar**: Pode reduzir a complexidade de espaço para `O(1)` já que precisamos apenas dos dois últimos valores.
    

```python
def climb_stairs_optimized(n):
    if n <= 2:
        return n
    
    prev2, prev1 = 1, 2  # Only store last two values
    for i in range(3, n + 1):
        current = prev1 + prev2
        prev2, prev1 = prev1, current
    return prev1
```

**Comparação de eficiência**

- **Recursivo ingênuo**: 9 chamadas de função para `n=5`, crescimento exponencial.
    
- **Tabulação**: 3 adições simples para `n=5`, crescimento linear.
    
- **Complexidade de tempo**: `O(n)` em vez de `O(2^n)`.
    
- **Complexidade de espaço**: `O(n)` para o array ou `O(1)` com otimização.
    
- **Desempenho previsível**: Sem risco de estouro de pilha para entradas grandes.
    

Ambas as abordagens reduzem a complexidade de tempo de exponencial `O(2^n)` para linear `O(n)`, uma melhoria dramática que faz a diferença entre resolver o problema em milissegundos e esperar anos para entradas maiores.

## Aplicações no Mundo Real

Programação dinâmica tem aplicações amplas em ciência da computação e além:

- **Otimização de Rotas**: sistemas de GPS usam algoritmos de programação dinâmica para encontrar os caminhos mais curtos entre os locais.
    
- **Processamento de Texto**: Verificadores ortográficos e recursos de autocompletar frequentemente dependem de programação dinâmica para calcular distâncias de edição entre palavras.
    
- **Modelagem Financeira**: Estratégias de investimento e otimização de portfólio frequentemente empregam técnicas de programação dinâmica.
    
- **Alocação de Recursos**: O problema da mochila e suas variantes aparecem em agendamento, orçamento e gerenciamento de recursos.
    

## Exemplo Prático: Problema da Troca de Moedas

O problema da troca de moedas é um desafio clássico de programação que, quando resolvido usando programação dinâmica, demonstra ambos os princípios-chave da PD: subestrutura ótima e subproblemas sobrepostos.

O problema da troca de moedas pergunta: "Qual é o número mínimo de moedas necessárias para atingir um valor alvo?"

Aqui está uma solução usando programação dinâmica:

```python
def min_coins(amount, coins):
    """Find minimum number of coins needed to make the given amount"""
    # Initialize dp array with "infinity" - represents impossible to make
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0  # Base case: 0 coins needed for amount 0
    
    # For each amount from 1 to target amount
    for i in range(1, amount + 1):
        # Try each coin denomination
        for coin in coins:
            if coin <= i:  # Can only use coin if it doesn't exceed current amount
                # Update minimum: current minimum vs (coins for remaining amount + 1)
                dp[i] = min(dp[i], dp[i - coin] + 1)
    
    # Return result if possible, -1 if impossible
    return dp[amount] if dp[amount] != float('inf') else -1

# Example usage:
# coins = [1, 3, 4], amount = 6
# dp[6] = min(dp[5]+1, dp[3]+1, dp[2]+1) = min(3+1, 1+1, 2+1) = 2
# Result: 2 coins (3 + 3)
```

E aqui está como o algoritmo de programação dinâmica para troca de moedas funciona passo a passo para `coins = [1, 3, 4]`, `amount = 6`:

```md
Initial state:
dp = [0, ∞, ∞, ∞, ∞, ∞, ∞]
     [0, 1, 2, 3, 4, 5, 6] ← amounts

Building up the solution:

For amount = 1:
  Try coin 1: dp[1] = min(∞, dp[0] + 1) = min(∞, 0 + 1) = 1
  dp = [0, 1, ∞, ∞, ∞, ∞, ∞]

For amount = 2:
  Try coin 1: dp[2] = min(∞, dp[1] + 1) = min(∞, 1 + 1) = 2
  dp = [0, 1, 2, ∞, ∞, ∞, ∞]

For amount = 3:
  Try coin 1: dp[3] = min(∞, dp[2] + 1) = min(∞, 2 + 1) = 3
  Try coin 3: dp[3] = min(3, dp[0] + 1) = min(3, 0 + 1) = 1
  dp = [0, 1, 2, 1, ∞, ∞, ∞]

For amount = 4:
  Try coin 1: dp[4] = min(∞, dp[3] + 1) = min(∞, 1 + 1) = 2
  Try coin 3: dp[4] = min(2, dp[1] + 1) = min(2, 1 + 1) = 2
  Try coin 4: dp[4] = min(2, dp[0] + 1) = min(2, 0 + 1) = 1
  dp = [0, 1, 2, 1, 1, ∞, ∞]

For amount = 5:
  Try coin 1: dp[5] = min(∞, dp[4] + 1) = min(∞, 1 + 1) = 2
  Try coin 3: dp[5] = min(2, dp[2] + 1) = min(2, 2 + 1) = 2
  Try coin 4: dp[5] = min(2, dp[1] + 1) = min(2, 1 + 1) = 2
  dp = [0, 1, 2, 1, 1, 2, ∞]

For amount = 6:
  Try coin 1: dp[6] = min(∞, dp[5] + 1) = min(∞, 2 + 1) = 3
  Try coin 3: dp[6] = min(3, dp[3] + 1) = min(3, 1 + 1) = 2
  Try coin 4: dp[6] = min(2, dp[2] + 1) = min(2, 2 + 1) = 2
  dp = [0, 1, 2, 1, 1, 2, 2]

Final result: dp[6] = 2 (achieved with coins 3 + 3)
```

Esta solução demonstra ambos os princípios-chave da programação dinâmica. Ela possui subproblemas sobrepostos porque encontrar as moedas mínimas para o valor 6 requer conhecer as soluções para os valores 5, 3 e 2. Esses mesmos subproblemas aparecem ao calcular outros valores. Ela possui subestrutura ótima porque a solução ótima para qualquer valor incorpora soluções ótimas para valores menores. Se soubermos que as moedas mínimas para o valor 3 são 1, então uma forma de fazer o valor 6 é usar essa solução mais uma moeda a mais de valor 3.

Sem DP, precisaríamos tentar todas as combinações possíveis de moedas - um número exponencial de possibilidades. Com DP, construímos a solução sistematicamente:

- **Complexidade de tempo**: `O(amount × number of coins)` em vez de exponencial.
    
- **Complexidade espacial**: `O(amount)` para o array `dp`.
    
- **Sem trabalho redundante**: Cada subproblema (encontrar o mínimo de moedas para cada valor) é resolvido exatamente uma vez.
    
- **Resultados reutilizáveis**: Uma vez que sabemos as moedas mínimas para o valor 3, usamos esse conhecimento para todos os valores maiores que podem se beneficiar dele.
    

## Quando Usar Programação Dinâmica

Programação dinâmica é eficaz quando:

- O problema pode ser dividido em subproblemas sobrepostos.
    
- O problema apresenta subestrutura ótima.
    
- Uma solução recursiva ingênua envolveria cálculos repetidos.
    
- Você precisa otimizar para complexidade de tempo ao custo da complexidade de espaço.
    

Padrões comuns de programação dinâmica incluem problemas de otimização (encontrar valores mínimos/máximos), problemas de contagem (número de maneiras de alcançar algo) e problemas de decisão que podem ser divididos em decisões menores.

A programação dinâmica transforma problemas complexos em problemas gerenciáveis ao armazenar e reutilizar sistematicamente soluções para subproblemas. Compreender essa técnica abre a porta para resolver uma ampla gama de desafios computacionais de forma eficiente.

# --questions--

## --text--

Quais são as duas condições principais que devem estar presentes para que a programação dinâmica seja aplicável?

## --answers--

Execução rápida e baixo uso de memória.

### --feedback--

Pense no que torna um problema adequado para otimização por programação dinâmica.

---

Subproblemas sobrepostos e subestrutura ótima.

---

Chamadas recursivas e loops iterativos.

### --feedback--

Pense no que torna um problema adequado para otimização por programação dinâmica.

---

Memorização e tabulação.

### --feedback--

Pense no que torna um problema adequado para otimização por programação dinâmica.

## --video-solution--

2

## --text--

Qual é a principal diferença entre as abordagens de memoization e tabulation em programação dinâmica?

## --answers--

A memoização usa mais memória do que a tabulação.

### --feedback--

Considere a direção na qual cada abordagem constrói a solução.

---

Tabulação é sempre mais rápida do que memoização.

### --feedback--

Considere a direção na qual cada abordagem constrói a solução.

---

Memoização é de cima para baixo enquanto tabulação é de baixo para cima.

---

A memoização funciona apenas com funções recursivas.

### --feedback--

Considere a direção na qual cada abordagem constrói a solução.

## --video-solution--

3

## --text--

Na implementação ingênua recursiva de subir escadas, qual é a complexidade de tempo e por que ela é ineficiente?

## --answers--

`O(n)` porque calcula cada passo uma vez.

### --feedback--

Pense em quantas vezes as mesmas contagens de degraus são calculadas na árvore recursiva.

---

`O(n²)` porque usa loops aninhados.

### --feedback--

Pense em quantas vezes as mesmas contagens de degraus são calculadas na árvore recursiva.

---

`O(2^n)` porque ele recalcula os mesmos subproblemas várias vezes.

---

`O(log n)` porque ele divide o problema ao meio.

### --feedback--

Pense em quantas vezes as mesmas contagens de degraus são calculadas na árvore recursiva.

## --video-solution--

3
